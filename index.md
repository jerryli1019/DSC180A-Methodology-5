**Name:** Yi Li, **Email:** yil115@ucsd.edu

**Section:** B17, **Mentor:** Hao Zhang

**Question 1:** What is the most interesting topic covered in your domain this quarter?

**Answer:** This quarter, the most captivating subject in my domain has been the calculation of memory and the number of FLOPs necessary for training a GPT-2 model. This is crucial because understanding the memory and FLOP requirements enables us to more effectively allocate GPU resources for training large language models. It also opens avenues for optimizing model efficiency and scaling capabilities.

**Question 2:** Describe a potential investigation you would like to pursue for your Quarter 2 Project.

**Answer:** For the upcoming Quarter 2 Project, we aim to extend the context length in the existing Vicuna project. This enhancement will allow the Vicuna model to process longer inputs than it currently does. While ChatGPT excels in this aspect, Vicuna's open-source nature makes it a valuable resource for researchers seeking to delve deeper into the theoretical underpinnings of large language models.

**Question 3:** What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?

**Answer:** A significant modification for the Quarter 1 Project would involve adjusting the number of parameters and tokens. This adjustment is intended to find the most optimal method for allocating GPU resources, thereby enhancing the efficiency and performance of the model under development.

**Question 4:** What other techniques would you be interested in using in your project?

**Answer:** I am also interested in exploring parallel computing applications in large language models. As the number of layers and blocks in transformer-based architectures increases, the role of parallel computing becomes increasingly critical for managing complexity and improving processing speed. This exploration could lead to more advanced and efficient model architectures.
